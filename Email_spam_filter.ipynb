{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Most Common Words Extraction\n",
    "\n",
    "The dataset selected is classified into 6 subdirectories, each with a ham and a spam directories. Each on contains ham and spam **.txt** files respectively. By using the **os** library, the dataset was filtered to a single dictionary containing the words with their respective number of occurance in all files. \n",
    "\n",
    "This was done by spliting each file's content to individual strings and adding them to a list. Next, a counter function was used to make count each word occurance in that list.\n",
    "\n",
    "It was found that there are special characters in the spam **.txt** files that the UTF-8 encoding does not classify as strings. To ignore this error, latin encoding was used to ensure that every character is treated as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "pBIRsXF3AJWm",
    "outputId": "baa5cf51-371e-48c6-c4f1-bfa421862dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 25656), ('to', 20345), ('ect', 13900), ('and', 12829), ('for', 10508)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob, os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.svm import SVC, NuSVC, LinearSVC\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "rootdir = \"Dataset\\enron1\" #Insert directory here respective to the notebook file\n",
    "\n",
    "def MostCommonWords(N, wordList, filesContent):\n",
    "    for directories, subdirectories, files in os.walk(rootdir):\n",
    "        for filename in files:      \n",
    "            with open(os.path.join(directories, filename),encoding=\"latin-1\") as f:\n",
    "                content = f.read()\n",
    "                filesContent += content\n",
    "                wordsInFile = content.split()\n",
    "                wordList+= wordsInFile\n",
    "\n",
    "    dictionary = Counter(wordList)\n",
    "    for word in list(dictionary):\n",
    "        if word.isalpha() == False: \n",
    "            del dictionary[word]\n",
    "        elif len(word) == 1:\n",
    "            del dictionary[word]\n",
    "\n",
    "    dictionary = dictionary.most_common(N)\n",
    "    return (dictionary)\n",
    "\n",
    "filesContent =[] #List containing each file's content\n",
    "wordList = [] #List containing all words used, with duplicates\n",
    "mostCommonWordsCount = 5 #Desired number of the most occurring words in the output dictionary\n",
    "\n",
    "commonWordsDictionary = (MostCommonWords(mostCommonWordsCount, wordList, filesContent))\n",
    "print(commonWordsDictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2  - Feature Extraction\n",
    "\n",
    "The process of feature extraction is one which a given set of data is reduced from a higher dimension to the lower dimension to reduce the processing power and save resources when it comes to doing data analysis.\n",
    "Here in this dataset, the emails were filtered to be in a two-dimension format (i.e dictionary) with the most commonly used **N** words. One can visualize such data into a huge, one-dimension vector or list of integers, each representing the number of occurances of words relevant to the dictionary, in a given email. \n",
    "\n",
    "For example, the dictionary output had the word \"**is**\" among the most occurring words in the email. Ultimately, if one email contained the message \"Hi, my name is Hossam Elghamry\", then the N-sized vector will be outputting [0,0,0...1,0,0,0]. The ***1*** integer value is the number of occurances of the \"**is**\" word in that given email, with the integer's position in the list being respective to that of the \"**is**\" in the dictionary.\n",
    "\n",
    "That said, we can represent all the feature extraction vectors in one ***MxN*** matrix. ***M*** being the number of emails processed and ***N*** being the count of the most occurring words (i.e the length of the output dictionary in **Part 1**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.]\n",
      " [19. 21.  1.  1.  5.]\n",
      " [ 2.  7.  1.  1.  1.]\n",
      " ...\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "def feature_extraction(): \n",
    "    features_matrix = np.zeros((len(filesContent),5)) #Making a (number of emails X number of most occuring words) matrix\n",
    "    fileID = 0; #Row incrementation value\n",
    "    for directories, subdirectories, files in os.walk(rootdir):\n",
    "        for filename in files:      \n",
    "            with open(os.path.join(directories, filename),encoding=\"latin-1\") as f:\n",
    "                words = f.read().split()\n",
    "                for word in words: #for every word in every file\n",
    "                    wordID = 0 \n",
    "                    for i,d in enumerate(commonWordsDictionary): #search for it in the dictionary of repeated words\n",
    "                        if d[0] == word:\n",
    "                        #if found, put the number of occurances of that current word by\n",
    "                        #using Count() function, in the feature matrix's word column, respective to the document's row\n",
    "                            wordID = i\n",
    "                            features_matrix[fileID,wordID] = words.count(word) \n",
    "        fileID = fileID + 1 #Next document index (i.e next row in the matrix)\n",
    "    return features_matrix\n",
    "\n",
    "print(feature_extraction())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Training Classifiers\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Welcome To Colaboratory",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
